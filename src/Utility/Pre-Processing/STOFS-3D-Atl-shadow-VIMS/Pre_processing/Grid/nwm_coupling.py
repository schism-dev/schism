'''
This script couples the NWM hydrofabric with a SCHISM grid.
The watershed rivers of the SCHISM grid need to be generated by RiverMapper,
which records extra river information in the z-field of the total_river_arcs_extra.map.

Tasks:
- Identify river elements in the SCHISM grid

- Calculate cross-sectional area at the head of all order-N rivers,
    where N is the lowest river order retained as the thalwegs for the RiverMapper.
    In sensitivity tests, a constant flow velocity is assumed for all rivers,
    so the volume source is calculated as the product of the cross-sectional area and the flow velocity.

- Find the source locations at the head of all order-N rivers.
    The order information is from NWM hydrofabric.
    "streamflow" from the NWM product is to be used as the volume source.

- Find the source locations at the mid-points of all thalwegs (NWM segments)

Note: a source location is mapped to the nearest center channel element of a river
'''

import os
from glob import glob
from datetime import datetime
from copy import deepcopy

import numpy as np
import xarray as xr
import geopandas as gpd
from sklearn.neighbors import KDTree
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from matplotlib import pyplot as plt

from pylib_experimental.schism_file import cread_schism_hgrid, TimeHistory, source_sink
from pylib import read_schism_bpfile, schism_bpfile
from RiverMapper.SMS import SMS_MAP
from RiverMapper.SMS import dl_lonlat2cpp
from RiverMapper.util import z_decoder


def b_in_a(a=None, b=None):
    '''
    Given two arrays A and B, return the indices of A's elements in B.
    '''

    if a is None and b is None:
        print('Demonstration of b_in_a')
        a = np.array([3, 5, 7, 1, 9, 8, 6, 6])
        b = np.array([3, 1, 5, 8, 6])
        print(f'A: {a}')
    else:
        a = np.array(a)
        b = np.array(b)

    index = np.argsort(a)
    sorted_a = a[index]
    sorted_index = np.searchsorted(sorted_a, b)

    bindex = np.take(index, sorted_index, mode="raise")
    mask = a[bindex] != b

    result = np.ma.array(bindex, mask=mask)
    return result


class Rivers():
    '''
    Class to handle river information from RiverMapper's output: total_river_arcs_extra.map,
    which contains extra information in the z-field.
    It groups river arcs into rivers and provides methods to manipulate river information.
    '''
    def __init__(self, river_arc_info: SMS_MAP, crs='epsg:4326') -> None:
        self.river_arc_info = river_arc_info  # original river arc information in an SMS_MAP object

        self.river_arc_gdf = river_arc_info.to_GeoDataFrame()  # river arcs in a GeoDataFrame
        self.river_arc_gdf.set_crs(crs, inplace=True)

        # list of river arcs grouped by rivers
        self.rivers_arcs, self.riverarcs_grouping = self.group_river_arcs()

        # index of rivers that are in the region of interest
        self.idx = np.ones(len(river_arc_info.arcs), dtype=bool)  # all rivers initially

        self.arcs_z = None  # z values of river arcs' vertices
        self.rivers_coor = None

        self.rivers_transects = None
        self.rivers_centerline_coor = None

        self.riverhead_transects = None
        self.riverhead_transects_center = None

    def group_river_arcs(self, nrow_info_position: int = 0) -> list:
        '''
        Arrange river information into a list of grouped river arcs.
        Each group corresponds to a river segment.

        river_arc_info: SMS_MAP object with river arc information coded in the z-field
        nrow_info_position: position of the z-field to decode the number of cross-channel nodes
            e.g., z=2.0401 and nrow_row_info_position=0:
            2 means there are 2 pieces of information in the z-field,
            "04" means 4 rows of arcs (including bank arcs) parrallel to the banks,
            "01" is another piece of information, in this case an indicator of outer arc
        '''
        n_arc = 0
        group = []
        group_idx = []
        while n_arc < len(self.river_arc_info.arcs):
            # nrow (number of arcs) in the cross-channel direction,
            # coded as the first two decimal digits of the z-field.
            # nrow is the same for all points along an arc, so just use the first one
            nrow = z_decoder(self.river_arc_info.arcs[n_arc].points[:, -1])[0][nrow_info_position]

            group.append(self.river_arc_info.arcs[n_arc:n_arc+nrow])
            group_idx.append(np.arange(n_arc, n_arc+nrow))

            n_arc += nrow
        return group, group_idx

    def get_river_arcs_z(self) -> list:
        '''get z values of river arcs vertices from the mesh'''
        raise NotImplementedError('Not implemented yet, use mesh_dp2riverarc_z() instead')

    def get_rivers_coor(self) -> list:
        '''get river vertices coordinates (x,y,z) grouped by rivers'''
        if self.arcs_z is None:
            self.get_river_arcs_z()

        rivers_coor = [None] * len(self.riverarcs_grouping)
        for k, arcs_ids in enumerate(self.riverarcs_grouping):
            if any(self.idx[arcs_ids]):
                rivers_coor[k] = np.stack([
                    np.c_[self.river_arc_info.arcs[i].points[:, :2], self.arcs_z[i]]
                    for i in arcs_ids
                ])
        self.rivers_coor = rivers_coor
        return rivers_coor

    def get_rivers_transects(self) -> list:
        '''get the cross-sections of rivers grouped by rivers'''

        if self.arcs_z is None:
            self.get_river_arcs_z()

        self.rivers_transects = [None] * len(self.riverarcs_grouping)
        for k, arcs_ids in enumerate(self.riverarcs_grouping):
            if any(self.idx[arcs_ids]):
                river_points = np.stack([
                    np.c_[self.river_arc_info.arcs[i].points[:, :2], self.arcs_z[i]]
                    for i in arcs_ids
                ])
                self.rivers_transects[k] = np.array([
                    river_points[:, i, :] for i in range(river_points.shape[1])
                ])  # transects along the river; i is along-river index
        return self.rivers_transects

    def get_rivers_centerline(self) -> list:
        ''' get river center coordinates grouped by rivers '''
        self.rivers_centerline_coor = []
        for _, river_arcs in enumerate(self.rivers_arcs):
            self.rivers_centerline_coor.append(np.mean([arc.points for arc in river_arcs], axis=0))

        return self.rivers_centerline_coor

    def mesh_dp2riverarc_z(self, hgrid_obj) -> list:
        '''
        get z from the nearest mesh nodes;
        most river arc points are mesh nodes except for those subject to cleaning

        :return: arcs_z: list of z values of river arcs' vertices
        '''
        xyz, l2g = self.river_arc_info.get_xyz()
        _, map2mesh = KDTree(np.c_[hgrid_obj.x, hgrid_obj.y]).query(xyz[:, :2])
        arcs_z = []
        for arc_idx in l2g:
            arcs_z.append(hgrid_obj.z[map2mesh[arc_idx]])
        self.arcs_z = arcs_z
        return arcs_z

    def set_region_of_interest(self, region_gdf: gpd.GeoDataFrame) -> np.ndarray:
        '''
        Find the index of rivers that are in the region of interest.
        Indices of the points inside region are saved in self.idx
        '''
        in_region = gpd.sjoin(
            self.river_arc_gdf.to_crs(region_gdf.crs), region_gdf,
            how='inner', predicate='intersects'
        ).index
        idx = np.zeros(len(self.river_arc_info.arcs), dtype=bool)
        idx[in_region] = True
        self.idx = idx

    def dredge_inner_arcs(
        self, min_channel_depth=1, region_gdf=None, diag_output_dir=None
    ) -> np.ndarray:
        '''
        Dredge the inner longitudinal transects based on the difference between
        the highest bank elevation and a user-defined depth
        Use the original river_arc_*.map as inputs to retain the original river arc order
        Use the valid_idx to filter out the rivers that are not in the region of interest

        :param min_channel_depth: float, depth to dredge the inner transects,
             measured from the higher bank
        :param region_gdf: gpd.GeoDataFrame, region of interest
        :param diag_output_dir: str, directory to save diagnostic files

        :return: dredged_points: np.ndarray, shape=(n_points, 3),
            x, y, z coordinates of dredged points
        '''
        if self.rivers_coor is None:
            self.get_rivers_coor()
        if region_gdf is not None:
            self.set_region_of_interest(region_gdf=region_gdf)

        dredged_points = np.zeros((0, 3), dtype=float)
        for k, arcs_id in enumerate(self.riverarcs_grouping):
            if any(self.idx[arcs_id]):
                # inside the watershed, i.e., where river arc points coorespond to mesh nodes

                # Measure target dp from the higher bank's dp.
                # rivers_coor: Left bank and right bank; along-river index; z of xyz
                bank_dp = np.min(self.rivers_coor[k][[0, -1], :, 2], axis=0)
                target_thalweg_dp = bank_dp + min_channel_depth  # target thalweg dp, positive downward

                # dredge the inner transects
                self.rivers_coor[k][1:-1, :, 2] = np.maximum(self.rivers_coor[k][1:-1, :, 2], target_thalweg_dp)
                dredged_points = np.r_[dredged_points, self.rivers_coor[k][1:-1, :, :].reshape(-1, 3)]

        # clip the dredged points to the watershed
        dredged_points_gdf = gpd.GeoDataFrame(
            geometry=gpd.points_from_xy(dredged_points[:, 0], dredged_points[:, 1], dredged_points[:, 2]),
            crs='EPSG:4326')
        # add z as a column
        dredged_points_gdf['z'] = dredged_points[:, 2]

        if region_gdf is not None:
            idx = gpd.sjoin(
                dredged_points_gdf.to_crs(region_gdf.crs), region_gdf, how='inner', predicate='intersects'
            ).index
            dredged_points = dredged_points[idx, :]
            dredged_points_gdf = dredged_points_gdf.iloc[idx]

        if diag_output_dir is not None:
            dredged_points_gdf.to_file(diag_output_dir + './dredged_points.shp')

        return dredged_points

    def match_transect(self, poi_xy: np.ndarray, diag_output_dir=None) -> tuple[list[np.ndarray], list[np.ndarray]]:
        '''
        find the nearest resolved river transect of each point of interest
        :param poi_xy: np.ndarray, shape=(n_points, 2), x, y coordinates of points of interest
        :param diag_output_dir: str, directory to save diagnostic files, None if not saving

        :return:
        riverhead_transects: list of np.ndarray, shape=(n_transect_points, 3),
           x, y, z coordinates of river head transects
        riverhead_transects_center: list of np.ndarray, shape=(3, ),
           x, y, z coordinates of river head transects center

        Note: not only used for river heads, but also for mid-points,
           i.e., only depending on how poi_xy is defined
           the variable naming needs to be changed
        '''

        rivercenter_coor = np.vstack(self.rivers_centerline_coor)
        # initialize the global to local mapping to -1
        rivercenter_vertices_g2l = np.zeros((rivercenter_coor.shape[0], 2), dtype=int) - 1
        n = 0
        for i in range(len(self.rivers_arcs)):  # group index
            # vertices index within a group (in this case only one arc, i.e., the centerline arc)
            for j in range(len(self.rivers_centerline_coor[i])):
                rivercenter_vertices_g2l[n] = [i, j]
                n += 1
        if n != len(rivercenter_coor):
            raise ValueError('Inconsistent number of river center vertices')

        _, idx = KDTree(rivercenter_coor[:, :2]).query(poi_xy)

        riverhead_transects = []
        riverhead_transects_center = []
        riverheads_in_arcgroup = rivercenter_vertices_g2l[np.squeeze(idx)]  # 2d array: [river index, vertices index]
        for i, indicies in enumerate(riverheads_in_arcgroup):  # riverhead: [river index, vertices index]
            river_idx, vertex_idx = indicies
            transect_coor = np.array([arc.points[vertex_idx, :] for arc in self.rivers_arcs[river_idx]])
            transect_z = np.array([self.arcs_z[arc_idx][vertex_idx] for arc_idx in self.riverarcs_grouping[river_idx]])
            riverhead_transects.append(np.c_[transect_coor[:, :2], transect_z])
            riverhead_transects_center.append(np.mean(transect_coor, axis=0))

        if diag_output_dir is not None:
            gpd.GeoDataFrame(
                geometry=gpd.points_from_xy(
                    x=np.vstack(riverhead_transects)[:, 0], y=np.vstack(riverhead_transects)[:, 1], crs='EPSG:4326')
            ).to_file(f'{diag_output_dir}/river_head_transect.shp')

        self.riverhead_transects = riverhead_transects
        self.riverhead_transects_center = riverhead_transects_center

        return riverhead_transects, riverhead_transects_center

    def match_river_center(self, poi_xy: np.ndarray) -> np.ndarray:
        '''find the nearest river center for each point of interest'''
        if self.rivers_centerline_coor is None:
            self.get_rivers_centerline()

        rivercenter_coor = np.vstack(self.rivers_centerline_coor)
        _, idx = KDTree(rivercenter_coor[:, :2]).query(poi_xy)

        return rivercenter_coor[np.squeeze(idx)]


def get_nwm_river_heads(nwm_gdf: gpd.GeoDataFrame) -> np.ndarray:
    '''
    Get river heads and mid-points from NWM hydrofabric.
    The river head is the upstream vertex of a river segment that has no "from" segment.

    : return: river_heads: np.ndarray, shape=(n_rivers, 2), x, y coordinates of river heads
    '''

    # drop all multistrings, which are from clipping
    # use copy to avoid SettingWithCopyWarning
    nwm_gdf = nwm_gdf[nwm_gdf['geometry'].apply(lambda x: x.geom_type == 'LineString')].copy()

    # add a 'from' column
    nwm_gdf.loc[:, 'from'] = 0
    for _, segment in nwm_gdf.iterrows():
        if int(segment['to']) > 0:
            idx = nwm_gdf['featureID'] == segment['to']  # find row indices of the 'to' segment
            if sum(idx) == 1:
                nwm_gdf.loc[idx, 'from'] = segment['featureID']  # last column is 'from'
            else:
                print(f"segment {segment['featureID']} has {sum(idx)} 'to'")

    # Find headwaters of the rivers (segments with 'from' == 0).
    # Identify the lowest-order river segments first to reduce the
    # number of segments we need to iterate over.
    lowest_order = min(nwm_gdf['order_'])
    lowest_order_rivers = nwm_gdf[nwm_gdf['order_'] == lowest_order]
    # initialize dict to store river head coordinates as values under featureID keys
    river_heads_dict = {}
    for _, segment in lowest_order_rivers.iterrows():
        if segment['from'] == 0:  # head of the river
            river_heads_dict[int(segment['featureID'])] = np.c_[
                segment.geometry.xy[0][0], segment.geometry.xy[1][0]
            ].squeeze()
    river_heads_xy = np.array(list(river_heads_dict.values())).squeeze()

    gdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy(river_heads_xy[:, 0], river_heads_xy[:, 1]), crs='EPSG:4326')
    gdf.to_file('./river_heads.shp')

    return river_heads_xy, river_heads_dict


def get_nwm_river_mid_points(nwm_gdf: gpd.GeoDataFrame) -> np.ndarray:
    '''
    get mid-points of the rivers from NWM hydrofabric
    The mid-point is defined as the mid-index of river vertices

    :param nwm_gdf: gpd.GeoDataFrame, NWM hydrofabric
    :return: mid_points: np.ndarray, shape=(n_rivers, 2), x, y coordinates of river mid-points
    '''

    # drop all multistrings, which are from clipping
    # use copy to avoid SettingWithCopyWarning
    nwm_gdf = nwm_gdf[nwm_gdf['geometry'].apply(lambda x: x.geom_type == 'LineString')].copy()

    mid_points_dict = {}
    for _, [_, segment] in enumerate(nwm_gdf.iterrows()):
        center_idx = int(len(segment.geometry.xy[0])/2)
        mid_points_dict[int(segment['featureID'])] = np.c_[
            segment.geometry.xy[0][center_idx], segment.geometry.xy[1][center_idx]
        ].squeeze()
    mid_points_xy = np.array(list(mid_points_dict.values())).squeeze()

    gdf = gpd.GeoDataFrame(
        geometry=gpd.points_from_xy(mid_points_xy[:, 0], mid_points_xy[:, 1]), crs='EPSG:4326')
    gdf.to_file('./mid_points.shp')

    return mid_points_xy, mid_points_dict


def neareast_neighbor(points: np.ndarray, target: np.ndarray) -> np.ndarray:
    ''' Find the nearest neighbor of each point in points from target '''
    tree = KDTree(target)
    _, idx = tree.query(points)
    return idx


def rearrange_river_arcs(source_river_arc_map: SMS_MAP, target_river_arc_map: SMS_MAP) -> SMS_MAP:
    '''
    Rearrange river_arc_*.map to match the order of the target_river_arc_map
    The target river_arc_map and the source river_arc_map to be processed must be from the same RiverMapper run.
    '''
    source_heads = np.array([arc.points[0, :] for arc in source_river_arc_map.arcs])
    target_heads = np.array([arc.points[0, :] for arc in target_river_arc_map.arcs])

    # use the river heads to find the corresponding river in the target
    _, target2source = KDTree(source_heads[:, :2]).query(target_heads[:, :2])
    target2source = np.squeeze(target2source)

    for i, target_arc in enumerate(target_river_arc_map.arcs):
        source_arc = source_river_arc_map.arcs[target2source[i]]
        if source_arc.points.shape[0] != target_arc.points.shape[0]:
            print(f'arc {i} inconsistent shape')
        if not np.allclose(source_arc.points[:, :2], target_arc.points[:, :2]):
            print(f'arc {i} inconsistent coordinates')

    river_arc_map_reordered = SMS_MAP(arcs=[source_river_arc_map.arcs[i] for i in target2source])
    return river_arc_map_reordered


def correct_transect_shape(rivers):
    '''
    correct transect shapes with machine learning techniques
    '''

    def normalize_rows_to_range(array, new_range=None):
        '''Normalize each row of a 2D array to a new range.'''

        if new_range is None:
            new_range = [0, 1]

        # Extract the new min and max from the range parameter
        new_min, new_max = new_range
        if new_min >= new_max:
            raise ValueError('new_min must be less than new_max')

        # Calculate the minimum and maximum of each row
        row_mins = array.min(axis=1, keepdims=True)
        row_maxs = array.max(axis=1, keepdims=True)

        # Apply the Min-Max scaling formula to each row
        denom = row_maxs - row_mins
        denom[denom == 0] = 1  # Prevent division by zero
        scaled_array = new_min + (array - row_mins) * (new_max - new_min) / denom

        return scaled_array

    # Example data
    a = -rivers.rivers_transects[4000][:, :, 2]
    a_scaled = normalize_rows_to_range(a, new_range=[0, 1])

    # Assume a is your dataset
    features = []
    for shape, shape_scaled in zip(a, a_scaled):
        # boundary_mean = np.mean([shape_scaled[0], shape_scaled[-1]])
        center_mean = np.mean(shape_scaled[1:-1])
        bank_thalweg_diff = max([shape_scaled[0], shape_scaled[-1]]) - center_mean
        left_bank_decent = shape_scaled[0] - shape_scaled[1]
        right_bank_decent = shape_scaled[-1] - shape_scaled[-2]
        mean_bias = np.mean(shape) - np.mean(a)
        features.append([bank_thalweg_diff, left_bank_decent, right_bank_decent, mean_bias])
    features = np.array(features)

    target = features
    # Isolation Forest
    iso = IsolationForest(contamination=0.15)  # assume 10% of data is outliers
    iso_pred = iso.fit_predict(target)
    print(iso_pred)

    # One-Class SVM
    ocsvm = OneClassSVM(nu=0.1, kernel='rbf', gamma=10)
    ocsvm.fit(target)
    svm_pred = ocsvm.predict(target)
    print(svm_pred)

    # One-Class SVM, iterate over gamma values
    desired = [1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1]
    gamma_values = np.logspace(-9, 3, 13)
    # Store the best model based on some evaluation metric
    best_model = None
    best_score = -np.inf  # example placeholder for your evaluation metric
    best_gamma = -np.inf
    for gamma in gamma_values:
        ocsvm = OneClassSVM(nu=0.1, kernel='rbf', gamma=gamma)
        ocsvm.fit(target)
        # Evaluate your model (e.g., via cross-validation)
        svm_pred = ocsvm.predict(target)
        score = sum(svm_pred == desired)  # Placeholder function
        if score > best_score:
            best_score = score
            best_model = ocsvm
            best_gamma = gamma

    # Predict with the best model
    print(f'best_gamma: {best_gamma}')
    svm_pred = best_model.predict(target)
    print(svm_pred)

    b = rivers.rivers_transects[4000]  # temporary, needs more work
    for i, shape in enumerate(b):
        if svm_pred[i] == -1:
            b[i, :, :] = b[i-1, :, :]

    # plot all transects from a river

    # ----------------- plot -----------------
    n_transects = len(rivers.rivers_transects[4000])
    _, ax = plt.subplots(n_transects, 1, figsize=(8, 6*n_transects))
    for i, transect in enumerate(rivers.rivers_transects[4000]):
        dl = np.r_[0, np.sqrt(np.diff(transect[:, 0])**2 + np.diff(transect[:, 1])**2).cumsum()]
        ax[i].plot(dl, -transect[:, 2])
    plt.show()

    _, ax = plt.subplots(n_transects, 1, figsize=(8, 6*n_transects))
    for i, transect in enumerate(b):
        dl = np.r_[0, np.sqrt(np.diff(transect[:, 0])**2 + np.diff(transect[:, 1])**2).cumsum()]
        ax[i].plot(dl, -transect[:, 2])
    plt.show()

# ------------------------------------------------
# ----------------- sample usage -----------------
# ------------------------------------------------


def make_constant_head_source(nwm_gdf: gpd.GeoDataFrame, rivers: Rivers, constant_velocity=1.0) -> np.ndarray:
    '''
    Make a source with a constant flow speed at the head of each river.

    :output: volume_sources_heads: np.ndarray, shape=(n_rivers, ), volume source at the head of each river
    '''
    # find the corresponding river transect of each river head (NWM hydrofabrics) in the domain
    # riverhead_transects_center (the second output) is not used here
    riverhead_transects, _ = rivers.match_river_heads(nwm_gdf)

    # calculate the cross-sectional area at the head
    river_head_transect_areas = np.zeros(len(riverhead_transects))
    for i, transect in enumerate(riverhead_transects):
        # level the transect as if it is filled with water
        transect[:, 2] -= min(transect[:, 2])
        transect[:, 2] = np.maximum(transect[:, 2], 0.5)  # set minimum depth to 0.5 m
        dl = np.r_[0, np.sqrt(np.diff(transect[:, 0])**2 + np.diff(transect[:, 1])**2).cumsum()]
        dl_cpp = dl_lonlat2cpp(dl, lat0=transect[0, 1])
        river_head_transect_areas[i] = np.trapz(transect[:, 2], x=dl_cpp)

    volume_sources_heads = river_head_transect_areas * constant_velocity

    return volume_sources_heads


def get_nwm_var(var_str="streamflow", nwm_files=None, fids=None):
    '''
    Get a variable from NWM
    :param var_str: str, variable name
    :param nwm_dir: list, NWM output files, each file contains a time step
    :param fids: np array or list, feature IDs of interest

    :return: var: np.ndarray, shape=(n_files, n_fids), variable values
    '''
    if nwm_files is None or len(nwm_files) == 0:
        raise ValueError('nwm_files is not provided')
    if fids is None or len(fids) == 0:
        raise ValueError('fids is not provided')

    fids = np.array(fids)

    river_heads_fid = np.array(list(fids)).astype(int)
    with xr.open_dataset(nwm_files[0]) as ds:
        fid = ds['feature_id'].values
        river_heads_idx = b_in_a(fid, river_heads_fid)

    var = np.zeros((len(nwm_files), len(river_heads_fid)))
    time_stamps = np.zeros((len(nwm_files), ), dtype=object)
    for i, file in enumerate(nwm_files):
        print(file)
        ds = xr.open_dataset(file)
        var[i, :] = ds[var_str].values[river_heads_idx]
        if len(ds['time']) != 1:
            raise ValueError('Multiple time steps in a file')
        time_stamps[i] = ds['time'].values[0]

        # deal with nan
        if np.any(np.isnan(var[i, :])):
            print(f'Warning: nan found in {var_str} at requested FIDs in {file} \
                   at time {time_stamps[i]} \
                   at {len(np.where(np.isnan(var[i, :]))[0])} locations')
            var[np.isnan(var)] = 0.0
            print('Warning: nan replaced with 0.0')

    return var, time_stamps


def make_interior_vsource(
    start_time,
    hgrid_obj, watershed_gdf, river_mapper_dir, nwm_gdf, nwm_files,
    heads=True, mid_points=True, output_dir=None
) -> source_sink:
    '''
    Make a source_sink object based on NWM, 
    which contains the volume source at the head of each river inside the domain
    and lateral sources at the mid-points of the rivers.

    Inputs:
        - heads: bool, whether to include river heads
        - mid_points: bool, whether to include mid-points of rivers

    Returns:
        - interior_ss: source_sink object
    '''

    poi_dict = {}
    if heads:
        poi_dict['river_head'] = {
            'find_method': get_nwm_river_heads,
            'nwm_var_str': 'streamflow'
        }
    if mid_points:
        poi_dict['mid_point'] = {
            'find_method': get_nwm_river_mid_points,
            'nwm_var_str': 'qSfcLatRunoff'
        }

    # -------------------- end inputs ----------------

    river_arc_info = SMS_MAP(f'{river_mapper_dir}/total_river_arcs_extra.map')
    rivers = Rivers(river_arc_info)
    rivers.set_region_of_interest(region_gdf=watershed_gdf)
    rivers.get_rivers_centerline()
    rivers.mesh_dp2riverarc_z(hgrid_obj)

    rivers.get_rivers_coor()
    rivers.get_rivers_transects()

    # find the corresponding river center element of each river head in NWM hydrofabrics
    hgrid_obj.compute_ctr()
    nt = len(nwm_files)
    total_volume_sources = np.zeros((nt, 0), dtype=float)
    total_vs_eleids = np.zeros((0, ), dtype=int)
    for poi_type, poi_info in poi_dict.items():
        print(f"Processing {poi_type}")
        xy, xy_dict = poi_info['find_method'](nwm_gdf)
        # find resolved transects
        _, transects_center = rivers.match_transect(poi_xy=xy, diag_output_dir=output_dir)
        # exclude transect centers outside hgrid
        in_grid_idx_mask = hgrid_obj.inside_grid(np.array([center[:2] for center in transects_center])).astype(bool)
        transects_center = np.array(transects_center)[in_grid_idx_mask]
        # match mesh elements
        _, poi_ele_ids = KDTree(np.c_[hgrid_obj.xctr, hgrid_obj.yctr]).query(np.array(transects_center)[:, :2])
        poi_ele_ids = np.squeeze(poi_ele_ids)
        # get volume source from NWM product, only for the elements inside the grid
        poi_fids = np.array(list(xy_dict.keys())).astype(int)[in_grid_idx_mask]
        # time_stamps should be the same for all types
        volume_sources, time_stamps = get_nwm_var(var_str=poi_info['nwm_var_str'], nwm_files=nwm_files, fids=poi_fids)

        # diagnostic output
        gdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy(
            hgrid_obj.xctr[poi_ele_ids], hgrid_obj.yctr[poi_ele_ids]
        ), crs='EPSG:4326')
        gdf['z'] = np.mean(volume_sources, axis=0)
        gdf.to_file(f'{output_dir}/{poi_type}_ele_vsources.shp')

        gdf = gpd.GeoDataFrame(geometry=gpd.points_from_xy( xy[:, 0], xy[:, 1]), crs='EPSG:4326')
        gdf.to_file(f'{output_dir}/{poi_type}_nwm_vsources.shp')

        total_volume_sources = np.c_[total_volume_sources, volume_sources]
        total_vs_eleids = np.r_[total_vs_eleids, poi_ele_ids]

    time_stamps = np.array(time_stamps, dtype='datetime64[s]')
    idx = time_stamps >= np.datetime64(start_time)
    time_stamps = time_stamps[idx]
    total_volume_sources = total_volume_sources[idx, :]
    time_seconds = (time_stamps - np.datetime64(start_time)).astype(int) / 1e6  # convert to seconds

    dummy_time = np.array([[0.0], [86400*1]])
    # _, idx = np.unique(total_vs_eleids, return_index=True)  # only retain unique elements
    # vs = TimeHistory(
    #     data_array=np.c_[time_seconds, np.tile(total_volume_sources[idx], (2, 1))],
    #     columns=(total_vs_eleids[idx] + 1).tolist(),  # +1 to convert to 1-based index, same as source_sink.in
    # )

    # make a source_sink object
    _, idx = np.unique(total_vs_eleids, return_index=True)  # only retain unique elements; todo, probably should add vsource from repeated elements
    vs = TimeHistory(
        data_array=np.c_[time_seconds, total_volume_sources[:, idx]],
        columns=(total_vs_eleids[idx] + 1).tolist(),  # +1 to convert to 1-based index, same as source_sink.in
        start_time_str=start_time.strftime('%Y-%m-%d %H:%M:%S')
    )
    ms_temperature = TimeHistory(
        data_array=np.c_[dummy_time, np.ones((2, len(total_vs_eleids[idx]))) * -9999],
        columns=(total_vs_eleids[idx] + 1).tolist(),
        start_time_str=start_time.strftime('%Y-%m-%d %H:%M:%S')
    )
    ms_salinity = TimeHistory(
        data_array=np.c_[dummy_time, np.zeros((2, len(total_vs_eleids[idx])))],
        columns=(total_vs_eleids[idx] + 1).tolist(),
        start_time_str=start_time.strftime('%Y-%m-%d %H:%M:%S')
    )
    interior_ss = source_sink(vsource=vs, vsink=None, msource=[ms_temperature, ms_salinity])
    os.makedirs(output_dir + '/Interior_ss/', exist_ok=True)
    interior_ss.diag_writer(hgrid_obj, output_dir=output_dir + '/Interior_ss/')
    return interior_ss


def generate_test_source_sink():
    '''
    Generate a constant source for river connectivity test so that each source is
    constant and equals to max(mean value, 1.0) * 10,
    which reflects a high constant flow condition

    # -------------------- inputs ----------------
    start_time = datetime(2021, 8, 1, 0, 0, 0)
    hgrid_obj = cread_schism_hgrid('/sciclone/schism10/feiye/STOFS3D-v8/I09d/hgrid.gr3')
    watershed_gdf = gpd.read_file('/sciclone/schism10/Hgrid_projects/STOFS3D-v8/'
                                  'v20p2s2v2/Clip/outputs/watershed.shp')
    river_mapper_dir = '/sciclone/schism10/Hgrid_projects/STOFS3D-v8/v20p2s2_RiverMapper/Outputs/' \
                      'bora_v20p2s2v8.LA_nwm_v1p2_5m_cleaned_20-core/'

    nwm_gdf = gpd.read_file('/sciclone/schism10/Hgrid_projects/STOFS3D-v8/'
                            'v20p2s2_RiverMapper/shapefiles/LA_nwm_v1p2_order2.gpkg')
    nwm_files = sorted(glob('/sciclone/schism10/feiye/STOFS3D-v8/I05a/Source_sink/'
                            'original_source_sink/20210801/nwm.t00z.medium_range.channel_rt_1.*.nc'))

    base_ss = source_sink.from_files(
        '/sciclone/schism10/feiye/STOFS3D-v8/I05a/Source_sink/original_source_sink/',
        start_time_str=start_time.strftime('%Y-%m-%d %H:%M:%S')
    )

    output_dir = '/sciclone/schism10/feiye/STOFS3D-v8/I05a/Source_sink/NWM_source_sink_v8/'
    # ------------------------------------------------

    # -------------------- inputs ----------------
    start_time = datetime(2021, 8, 1, 0, 0, 0)
    hgrid_obj = cread_schism_hgrid('/sciclone/schism10/feiye/STOFS3D-v8/I09d/hgrid.gr3')
    watershed_gdf = gpd.read_file('/sciclone/schism10/Hgrid_projects/STOFS3D-v8/'
                                  'v43s2_RiverMapper/v44/Clip/outputs/watershed.shp')
    river_mapper_dir = ('/sciclone/schism10/Hgrid_projects/STOFS3D-v8/'
                        'v43s2_RiverMapper/Outputs/femto.v44.v44_centerline_16-core/')

    nwm_gdf = gpd.read_file('/sciclone/schism10/Hgrid_projects/STOFS3D-v8/'
                            'v20p2s2_RiverMapper/shapefiles/LA_nwm_v1p2_order2.gpkg')
    nwm_files = sorted(glob('/sciclone/schism10/feiye/STOFS3D-v8/I09g/Source_sink/'
                            'original_source_sink/20210801/nwm.t00z.medium_range.channel_rt_1.*.nc'))

    base_ss = source_sink.from_files(
        '/sciclone/schism10/feiye/STOFS3D-v8/I09j/Source_sink/USGS_adjusted_sources/',
        start_time_str=start_time.strftime('%Y-%m-%d %H:%M:%S')
    )

    inject_heads = True
    inject_mid_points = True
    plumbing_test = False

    output_dir = '/sciclone/schism10/feiye/STOFS3D-v8/I09j/Source_sink/Bnd_interior_source_sink/'
    # ------------------------------------------------
    '''

    # -------------------- inputs ----------------
    start_time = datetime(2024, 3, 5, 0, 0, 0)
    hgrid_obj = cread_schism_hgrid('/sciclone/schism10/feiye/STOFS3D-v8/I09d/hgrid.gr3')
    watershed_gdf = gpd.read_file('/sciclone/schism10/Hgrid_projects/STOFS3D-v8/'
                                  'v43s2_RiverMapper/v44/Clip/outputs/watershed.shp')
    river_mapper_dir = ('/sciclone/schism10/Hgrid_projects/STOFS3D-v8/'
                        'v43s2_RiverMapper/Outputs/gulf.v45.v45_centerline_5-core')

    nwm_gdf = gpd.read_file('/sciclone/schism10/Hgrid_projects/STOFS3D-v8/'
                            'v20p2s2_RiverMapper/shapefiles/LA_nwm_v1p2_order2.gpkg')
    nwm_files = sorted(glob('/sciclone/schism10/feiye/STOFS3D-v8/I09/Source_sink/'
                            'original_source_sink/20240305/nwm.t00z.medium_range.channel_rt_1.*.nc'))

    base_ss = source_sink.from_files(
        '/sciclone/schism10/feiye/STOFS3D-v8/I09f4/Source_sink/original_source_sink/',
        start_time_str=start_time.strftime('%Y-%m-%d %H:%M:%S')
    )

    inject_heads = True
    inject_mid_points = True
    plumbing_test = False

    output_dir = '/sciclone/schism10/feiye/STOFS3D-v8/I09f4/Source_sink/Bnd_interior_source_sink0/'

    # ------------------------------------------------

    # save a copy of this script to the output directory
    os.makedirs(output_dir, exist_ok=True)
    os.system(f'cp {__file__} {output_dir}/')

    # Process
    interior_ss = make_interior_vsource(
        start_time,
        hgrid_obj, watershed_gdf, river_mapper_dir, nwm_gdf, nwm_files,
        heads=inject_heads, mid_points=inject_mid_points, output_dir=output_dir
    )

    combined_ss = base_ss + interior_ss

    if plumbing_test:  # post-process the source sink: clip and scale
        # force min flow before scaling
        combined_ss.vsource.df[:] = np.maximum(combined_ss.vsource.data, 1.0)
        # make constant and scale
        combined_ss.vsource.df[:] = np.tile(combined_ss.vsource.data.mean(axis=0)*10, reps=(combined_ss.vsource.n_time, 1))
        # force max flow, mean Mississippi flow is ~ 16800 cfs
        combined_ss.vsource.df[:] = np.minimum(combined_ss.vsource.data, 20000)

    # write the source sink files
    combined_ss.writer(output_dir=output_dir)
    combined_ss.diag_writer(hgrid_obj, output_dir=output_dir)

    print('Done writing source_sink files')


def generate_selected_source_sink():
    '''
    Generate a source sink object with selected rivers only
    '''

    # -------------------- inputs ----------------
    original_ss = source_sink.from_ncfile(
        '/sciclone/schism10/feiye/STOFS3D-v8/I09d/Source_sink/Connect_Test_source_sink/source.nc',
    )
    selected_points = gpd.read_file(
        '/sciclone/schism10/feiye/STOFS3D-v8/I09d/Source_sink/Selected_source_sink/'
        'selected_sources.shp'
    )
    hgrid_obj = cread_schism_hgrid(
        '/sciclone/schism10/feiye/STOFS3D-v8/I09d/hgrid.gr3'
    )
    # ------------------------------------------------

    destination_xy = np.array([selected_points.geometry.x, selected_points.geometry.y]).T
    destination_bp = schism_bpfile(x=destination_xy[:, 0], y=destination_xy[:, 1])
    destination_bp.save('/sciclone/schism10/feiye/STOFS3D-v8/I09d/Source_sink/Selected_source_sink/selected_sources.bp')

    mapped_ss = original_ss.map_source_ele(hgrid_obj=hgrid_obj, destination_xy=destination_xy)
    mapped_ss.writer('/sciclone/schism10/feiye/STOFS3D-v8/I09d/Source_sink/Selected_source_sink/')


def reposition_bp():
    '''sample usage of repositioning build points'''
    bp = read_schism_bpfile('/sciclone/schism10/feiye/STOFS3D-v8/BPfiles/USGS.bp')
    run_id = 'R20b'

    map_dict = {
        'R13_v8': (
            '/sciclone/schism10/Hgrid_projects/STOFS3D-v8/v46_RiverMapper/'
            'Outputs/gulf.v49.v49_centerline_16-core/total_river_arcs_extra.map'
        ),
        'R20b': (
            '/sciclone/schism10/Hgrid_projects/STOFS3D-v8/a50_RiverMapper/'
            'Outputs/gulf.a50.4.a50p4_32-core/total_river_arcs_extra.map'
        ),
    }

    rivers = Rivers(SMS_MAP(map_dict[run_id]))

    xy = rivers.match_river_center(np.c_[bp.x, bp.y])

    repositioned_bp = deepcopy(bp)
    repositioned_bp.x = xy[:, 0]
    repositioned_bp.y = xy[:, 1]
    repositioned_bp.save('/sciclone/schism10/feiye/STOFS3D-v8/BPfiles/USGS_repositioned_v50.bp')


def dredge_river_transects():
    '''
    Dredge the inner arcs of each river transect to maintain a minimum elevation drop
    from bank to thalweg, thus maintaining channel connectivity

    Inputs for specific cases:
    <I09>
    min_channel_depth = 2.0

    <I14>
    min_channel_depth = 1.0
    map_dir = ('/sciclone/schism10/Hgrid_projects/STOFS3D-v8/v43s2_RiverMapper/'
               'Outputs/femto.v44.v44_centerline_16-core')
    rivers = Rivers(SMS_MAP(f'{map_dir}/total_river_arcs_extra.map'))

    watershed = gpd.read_file(
        '/sciclone/schism10/Hgrid_projects/STOFS3D-v8/v43s2_RiverMapper/v44/Clip/'
        'outputs/watershed.shp')

    hgrid_obj = cread_schism_hgrid(
        '/sciclone/schism10/feiye/STOFS3D-v8/I14/Bathy_edit/RiverArc_Dredge/hgrid.ll')

    output_dir = '/sciclone/schism10/feiye/STOFS3D-v8/I14/Bathy_edit/RiverArc_Dredge/'

    <I10>
    min_channel_depth = 1.0
    map_dir = ('/sciclone/schism10/Hgrid_projects/STOFS3D-v8/v46_RiverMapper/'
               'Outputs/gulf.v49.v49_centerline_16-core/')
    rivers = Rivers(SMS_MAP(f'{map_dir}/total_river_arcs_extra.map'))

    watershed = gpd.read_file(
        '/sciclone/schism10/Hgrid_projects/STOFS3D-v8/v22/Clip/'
        'outputs/watershed.shp')

    hgrid_obj = cread_schism_hgrid(
        '/sciclone/schism10/feiye/STOFS3D-v8/I10/Bathy_edit/RiverArc_Dredge/hgrid.ll')

    output_dir = '/sciclone/schism10/feiye/STOFS3D-v8/I10/Bathy_edit/RiverArc_Dredge/'

    <I20>
    min_channel_depth = 1.5

    <I13z_v7>
    min_channel_depth = 0.6
    map_dir = (
        '/sciclone/schism10/Hgrid_projects/STOFS3D-v7/v19_RiverMapper/Outputs/'
        'bora_v19.1.v19_ie_v18_3_nwm_clipped_in_cudem_missing_tiles_20-core/'
    )
    rivers = Rivers(SMS_MAP(f'{map_dir}/total_river_arcs_extra.map'))

    watershed = gpd.read_file(
        '/sciclone/schism10/Hgrid_projects/STOFS3D-v7/v20.1/Clip/watershed.shp'
    )

    hgrid_obj = cread_schism_hgrid(
        '/sciclone/schism10/feiye/STOFS3D-v8/I13z_v7/Bathy_edit/RiverArc_Dredge/hgrid.ll')

    output_dir = '/sciclone/schism10/feiye/STOFS3D-v8/I13z_v7/Bathy_edit/RiverArc_Dredge/'

    <I24.4>
    min_channel_depth = 0.6
    map_dir = (
        '/sciclone/schism10/Hgrid_projects/STOFS3D-v8/a50_RiverMapper/'
        'Outputs/gulf.a50.4.a50p4_32-core/'
    )
    rivers = Rivers(SMS_MAP(f'{map_dir}/total_river_arcs_extra.map'))

    watershed = gpd.read_file(
        '/sciclone/schism10/Hgrid_projects/STOFS3D-v8/v24.4/Clip/'
        'outputs/watershed.shp'
    )

    <I27>
    min_channel_depth = 0.6
    map_dir = (
        '/sciclone/schism10/Hgrid_projects/STOFS3D-v7/v19_RiverMapper/Outputs/'
        'bora_v19.1.v19_ie_v18_3_nwm_clipped_in_cudem_missing_tiles_20-core/'
    )
    rivers = Rivers(SMS_MAP(f'{map_dir}/total_river_arcs_extra.map'))

    watershed_all = gpd.read_file(
        '/sciclone/schism10/Hgrid_projects/STOFS3D-v8/v27/Clip/outputs/watershed.shp'
    )
    watershed_exclusion = gpd.read_file(
        '/sciclone/schism10/feiye/STOFS3D-v8/I13y_v7/Bathy_edit/RiverArc_Dredge/watershed_ME.shp'
    ).to_crs(watershed_all.crs)
    watershed = gpd.overlay(watershed_all, watershed_exclusion, how='difference')

    hgrid_obj = cread_schism_hgrid(
        '/sciclone/schism10/feiye/STOFS3D-v8/I13y_v7/Bathy_edit/RiverArc_Dredge/hgrid.ll')

    output_dir = '/sciclone/schism10/feiye/STOFS3D-v8/I13y_v7/Bathy_edit/RiverArc_Dredge/'
    '''

    # <a19.1>
    min_channel_depth = 0.6
    rivers = Rivers(SMS_MAP(
        '/sciclone/schism10/Hgrid_projects/STOFS3D-v7/v19_RiverMapper/Outputs/'
        'bora_v19.1.v19_ie_v18_3_nwm_clipped_in_cudem_missing_tiles_20-core/'
        'total_river_arcs_extra.map'
    ))

    watershed_origional = gpd.read_file(
        '/sciclone/schism10/Hgrid_projects/STOFS3D-v8/v31/Clip/outputs/watershed.shp'
    )
    watershed = gpd.overlay(
        watershed_origional,
        gpd.read_file(
            '/sciclone/schism10/feiye/STOFS3D-v8/I15_v7/Bathy_edit/RiverArc_Dredge/watershed_ME.shp'
        ).to_crs(watershed_origional.crs),
        how='difference'
    )

    hgrid_obj = cread_schism_hgrid(
        '/sciclone/schism10/feiye/STOFS3D-v8/I15a_v7/Bathy_edit/RiverArc_Dredge/hgrid.ll')

    output_dir = '/sciclone/schism10/feiye/STOFS3D-v8/I15a_v7/Bathy_edit/RiverArc_Dredge/'

    # <a50.4>
    # min_channel_depth = 0.6
    # rivers = Rivers(SMS_MAP(
    #     '/sciclone/schism10/Hgrid_projects/STOFS3D-v8/a50_RiverMapper/Outputs/'
    #     'gulf.a50.4.a50p4_32-core/total_river_arcs_extra.map'
    # ))

    # watershed_origional = gpd.read_file(
    #     '/sciclone/schism10/Hgrid_projects/STOFS3D-v8/v29/Clip/outputs/watershed.shp'
    # )
    # watershed = gpd.overlay(
    #     watershed_origional,
    #     gpd.read_file(
    #         '/sciclone/schism10/feiye/STOFS3D-v8/I13y_v7/Bathy_edit/RiverArc_Dredge/watershed_ME.shp'
    #     ).to_crs(watershed_origional.crs),
    #     how='difference'
    # )

    # hgrid_obj = cread_schism_hgrid(
    #     '/sciclone/schism10/feiye/STOFS3D-v8/I13x/Bathy_edit/RiverArc_Dredge/hgrid.ll')

    # output_dir = '/sciclone/schism10/feiye/STOFS3D-v8/I13x/Bathy_edit/RiverArc_Dredge/'

    # ----------------------- inputs -----------------------
    # ----------------------------- end inputs -----------------------------

    rivers.mesh_dp2riverarc_z(hgrid_obj)
    dredged_points = rivers.dredge_inner_arcs(
        region_gdf=watershed, diag_output_dir=output_dir, min_channel_depth=min_channel_depth)

    # map dredged points to mesh nodes
    _, idx = KDTree(np.c_[hgrid_obj.x, hgrid_obj.y]).query(dredged_points[:, :2])

    # update the mesh
    hgrid_dredged = deepcopy(hgrid_obj)
    hgrid_dredged.dp[np.squeeze(idx)] = np.maximum(
        hgrid_dredged.dp[np.squeeze(idx)], dredged_points[:, 2]
    )
    hgrid_dredged.grd2sms(output_dir + '/hgrid_dredged.2dm')
    hgrid_dredged.save(output_dir + '/hgrid_dredged.gr3', fmt=1)


def view_nwm():
    '''View time-series of NWM variables at line segments (FIDs) of the hydrofabric'''
    # usgs2fid_dict = {
    #     '02492511': ['15707937', '15708315'],
    #     '02492600': ['15708755'],
    #     '07375050': ['18931936'],
    #     '07375170': ['18928090'],
    #     '07375500': ['18975531'],
    #     '07376000': ['20090368'],
    #     '07378050': ['18988626'],
    #     '07378500': ['18990204'],
    # }
    # inputs
    fids = ['15720717', '15707937', '15721007', '15707927', '15707961']
    fids = ['15148368', '21898499', '21898517']
    fids = ['15148144']
    fids = ['19406836']
    # nwm_files = sorted(glob('/sciclone/schism10/feiye/STOFS3D-v8/I03/Source_sink/'
    #                         'original_source_sink/20240305/nwm.t00z.medium_range.channel_rt_1.*.nc'))
    nwm_files = sorted(glob(
        '/sciclone/schism10/feiye/STOFS3D-v8/I13/Source_sink/original_source_sink/20171201/'
        '*.CHRTOUT_DOMAIN1.comp'
    ))
    # end inputs

    nwm, time_stamps = get_nwm_var(var_str="streamflow", nwm_files=nwm_files, fids=fids)

    # plot time series
    _, ax = plt.subplots()
    for i, fid in enumerate(fids):
        ax.plot(time_stamps, nwm[:, i], label=fid)
        ax.legend()
    plt.show()


if __name__ == '__main__':
    view_nwm()
    generate_test_source_sink()
    generate_selected_source_sink()
    dredge_river_transects()
    reposition_bp()

    print('Done')
