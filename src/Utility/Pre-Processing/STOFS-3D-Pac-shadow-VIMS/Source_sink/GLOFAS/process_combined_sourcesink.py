import numpy as np
import netCDF4 as nc
import os
import datetime
import argparse
import json
import glob

######################################
# model time range from param.nml

def get_aggregated_features(nc_feature_id, features):

    aggregated_features = []
    for source_feats in features:
        aggregated_features.extend(list(source_feats))

    in_file=[]
    for feature in aggregated_features:
        idx=np.where(nc_feature_id == int(feature))[0]
        in_file.append(idx.item())

    in_file_2 = []
    sidx = 0
    for source_feats in features:
        eidx = sidx + len(source_feats)
        #in_file_2.append(in_file[sidx:eidx].tolist())
        in_file_2.append(in_file[sidx:eidx])
        sidx = eidx
    return in_file_2

def streamflow_lookup_hindcast(file, indexes, threshold=-1e-5, timeind = None):
    ncid = nc.Dataset(file)
    if timeind is None:
       streamflow = ncid["streamflow"][:]  # forecast uses all times
    else:
        streamflow = ncid["streamflow"][timeind,:]  # hindcast doesn't necessarily
    streamflow[np.where(streamflow < threshold)] = 0.0
    #change masked value to zero
    streamflow = streamflow.filled(0.0)
    data = []
    for indxs in indexes:
        # Note: Dataset already consideres scale factor and offset.
        data.append(np.sum(streamflow[:,indxs],axis=1))
    ncid.close()
    return data

######################################
# model time range from param.nml
oper = False
if oper:
   parser = argparse.ArgumentParser(description='''Process GLOFAS and NWM output for source_sink files, uses preprocessed glofas and NWM location information.
         ''')
   parser.add_argument(dest='nwmdir', type=str, help='location of NWM .json files')
   parser.add_argument(dest='nwmfiles', type=str, help='NWM files, 1 file per region, argument format is reg:filename,reg:filename, regions specified here determine which json files are used')
   parser.add_argument(dest='glofasdir', type=str, help='location of preproc-glofas.npz')
   parser.add_argument(dest='glofasfiles', type=str, help='glofas files, comma separated list to allow composites across separate downloads')
   parser.add_argument(dest='outdir', type=str, help='location to write source_sink files')
   parser.add_argument('-s','--startdate',dest='startdate', default=None,type=str, help='run start YYYY-MM-DD, defaults to today')
   parser.add_argument('-r','--rndays',dest='rnday', default=5, type=int, help='length of run in days')
   parser.add_argument('-p','--plotbounds',dest='plotbounds',type=str, default='Columbia:235.5,238,44.5,47.5;VancIs:231,237,48.25,52.1',help='regions for sanity-check plots, only used if imagedir specified, format is "name1:minx,maxx,miny,maxy;name2...";')
   parser.add_argument('-i','--imagedir',dest='imagedir', type=str, default=None, help='location to place sanity check images, no images generated by default or if matplotlib is unavialable')
   parser.add_argument('-t','--tag',dest='tag', type=str, default='', help='suffix for static npz filename and output image name prefix, defaults to empty string, mainly useful for testing')
   parser.add_argument('-H','--hindcast',dest='fcast', action="store_false", help='use to specify Hindcast processing, omit for forecast processing')
   args = parser.parse_args()
   fcast = args.fcast
   outdir = args.outdir # source_sink files written here
   glofasdir = args.glofasdir # location of preproc-glofas.npz
   glofasfiles = args.glofasfiles.split(',') # glofas download data
   nwmdir = args.nwmdir # location of nwm json files
   nwmfiles = {}
   for f in args.nwmfiles.split(','): # nwm download data
      reg, file = f.split(':')
      nwmfiles[reg] = file
   #param = pl.read(paramfile)
   #modelstartdate = datetime.datetime(int(param['start_year']), int(param['start_month']),int(param['start_day']))
   #rnday = int(param['rnday'])
   if args.startdate is not None:
      mdate = [int(x) for x  in args.startdate.split('-')]
   else:
      now = datetime.datetime.now()
      mdate = [now.year,now.month,now.day]
   modelstartdate = datetime.datetime(mdate[0],mdate[1],mdate[2])
   rnday = args.rnday
   modelenddate = modelstartdate + datetime.timedelta(days=rnday)
   HImodelstartdate = modelstartdate # allows hindcast handling for no HI data in 2018
   HImodelenddate = modelenddate
   imagedir = args.imagedir # optional images written here if location is provided
   if imagedir is None:
       processplot = False
   else:
       processplot = True
   tag = args.tag
   intag = tag   # allows separation between glofas_prep_{tag}.npz filename and output filename tags
   plotstr = args.plotbounds
# hard-code values
processplot = True
imagedir = './'
outdir = 'source_sink/'
intag='NEW23-NWMconus500km'
fcast=False
nwmdir = 'json/'
nwmdata = 'nwmdata'
nwmfilesstr = 'alaska:NWMv3.0_alaska_2017-11-01-2019-01-01.nc,hawaii:NWMv3.0_hawaii_2012-11-01-2014-11-01.nc,conus:NWMv3.0_conus_2017-11-01-2019-01-01.nc'
nwmfiles = {}
for f in  nwmfilesstr.split(','):
    reg, file = f.split(':')
    nwmfiles[reg] = os.path.join('nwmdata',file)
modelstartdate = datetime.datetime(2017,11,1)
modelenddate = datetime.datetime(2018,1,1)
modelstartdate = datetime.datetime(2018,1,1)
modelenddate = datetime.datetime(2019,1,1)
HImodelstartdate = datetime.datetime(modelstartdate.year - 5,modelstartdate.month, modelstartdate.day)
HImodelenddate = datetime.datetime(modelenddate.year - 5,modelenddate.month, modelenddate.day)
glofasdir = 'glofasprep/'
glofasfiles = glob.glob(f'glofasdata/*{modelstartdate.year}*.nc')
tag=f'NEW23-NWMconus500km-{modelstartdate.year}'
plotstr='Columbia:235.5,238,44.5,47.5;VancIs:231,237,48.25,52.1'
# plotting areas for sanity checks
#plotbounds = {'Columbia':[235.5,238,44.5,47.5],'VancIs':[231,237, 48.25, 52.1]}
#plotarea = 'VancIs'
plotbounds = {}
if processplot:
   try:
      import matplotlib.pyplot as plt
      for bstr in plotstr.split(';'):
         name, bndstr = bstr.split(':')
         plotbounds[name] = np.array([float(x) for x in bndstr.split(',')])
   except ModuleNotFoundError:
      print('matplotlib not available, no plots will be generated')
      processplot = False
      plotbounds = {}

# prep variables for NWM data
sources_all = []
sinks_all = []
eid_sources = []
eid_sinks = []
times = []
dates = []
# read in discharge from NWM
for reg in ['conus']: #nwmfiles.keys():
    fn = os.path.join(nwmdir,f'NWM_outside_{reg}_clean.json')
    with open(fn) as f: outside_fid = json.load(f)
    fn = os.path.join(nwmdir,f'sources_{reg}_global.json')
    with open(fn) as f: sources_fid = json.load(f)
    fn = os.path.join(nwmdir,f'sinks_{reg}_global.json')
    with open(fn) as f: sinks_fid = json.load(f)
    files = [nwmfiles[reg]]
    print(f'reg is {reg}')
    #merge outside sources and crossing sources, checking for repeat elements
    overlap = set(outside_fid.keys()).intersection(sources_fid.keys())
    if overlap == set():
        sources_fid = sources_fid | outside_fid # merge dictionaries
    else:
        tmp = sources_fid | outside_fid
        for k in overlap:
            for v in sources_fid[k]: tmp[k].append(v) 
        sources_fid = tmp
    #add to the final list
    eid_sources.extend(list(sources_fid.keys()))
    eid_sinks.extend(list(sinks_fid.keys()))
    nc_fid0 = nc.Dataset(files[0])["feature_id"][:]
    src_idxs = get_aggregated_features(nc_fid0, sources_fid.values())
    snk_idxs = get_aggregated_features(nc_fid0, sinks_fid.values())
    sources = []
    sinks = []
    # need handling for forecast filenames for NWM from NWM/Combined% vi gen_sourcesink.py
    for fname in files:
        ds = nc.Dataset(fname)
        ncfeatureid=ds['feature_id'][:]
        if not np.all(ncfeatureid == nc_fid0):
            print(f'Indexes of feature_id are changed in  {fname}')
            src_idxs=get_aggregated_features(ncfeatureid, sources_fid.values())
            snk_idxs=get_aggregated_features(ncfeatureid, sinks_fid.values())
            nc_fid0 = ncfeatureid
        if fcast:
           sources.append(streamflow_lookup(fname, src_idxs))
           sinks.append(streamflow_lookup(fname, snk_idxs))
           model_time = datetime.datetime.strptime(ds.model_output_valid_time, "%Y-%m-%d_%H:%M:%S")
           if reg == 'conus':
              dates.append(str(model_time))
              times.append((model_time - startdate).total_seconds())
        else:
           if reg == 'hawaii': # use 2012, data is per minute
              nwmstarttime = datetime.datetime.strptime(ds.variables['time'].units, "minutes since %Y-%m-%dT%H:%M:%S")
              timescaler = 60
              relmodend = (HImodelenddate - nwmstarttime).total_seconds()/timescaler
              relmodstart = (HImodelstartdate - nwmstarttime).total_seconds() / timescaler
           else:
              nwmstarttime = datetime.datetime.strptime(ds.variables['time'].units, "hours since %Y-%m-%dT%H:%M:%S")
              timescaler = 3600
              relmodend = (modelenddate - nwmstarttime).total_seconds()/timescaler
              relmodstart = (modelstartdate - nwmstarttime).total_seconds() / timescaler
           timeind = np.logical_and(ds.variables['time'][:] >= relmodstart, ds.variables['time'][:] <=relmodend)
           sources = streamflow_lookup_hindcast(fname, src_idxs,timeind = timeind)
           sinks = streamflow_lookup_hindcast(fname, snk_idxs,timeind=timeind)
           if reg == 'conus':
              nwmtimes = (ds.variables['time'][timeind] - relmodstart)*timescaler
              nwmdates = [str(modelstartdate + datetime.timedelta(seconds = d)) for d in times]
        ds.close()
    if reg == 'hawaii': # historical data is 15 minute, subsample
       sources_all.append( np.array(sources)[:,::4])
       sinks_all.append( np.array(sinks)[:,::4])
    else:
       sources_all.append( np.array(sources))
       sinks_all.append( np.array(sinks))
# merge across the three regions
#nwmsources = np.concatenate((sources_all[0], sources_all[1], sources_all[2]), axis=0).T
#nwmsinks = np.concatenate((sinks_all[0], sinks_all[1], sinks_all[2]), axis=0).T
nwmsources = sources_all[0].T
nwmsinks = sinks_all[0].T
# need to merge these with the final outputs of GLOFAS processing

#need to interpolate GLOFAS outputs to NWM times

# need to check that NWM times are consistent, Hawaii had times in minutes, possibly just because it has a start time of 00:15

preproc = np.load(os.path.join(glofasdir,f'glofas_prep{intag}.npz'))
Erivers=preproc["Erivers"];idxs=preproc["idxs"];ind0N=preproc["ind0N"];ind1N=preproc["ind1N"]; lonN=preproc["lonN"];latN=preproc["latN"];lon=preproc["lon"];lat=preproc["lat"];areaN=preproc["areaN"];final=preproc["final"]
xctr = preproc["xctr"]
yctr= preproc["yctr"]
elnode = preproc["elnode"]
ne = preproc["ne"]
uniqErivers=preproc['uniqErivers']
singles=preproc['singles']
uniqEriversInd=preproc['uniqEriversInd'] 
dups=preproc['dups']

# read in spatial subsets of GLOFAS data
riverCoordsSubset = np.zeros((Erivers.shape[0],2))
discharge = None # set after reading first file and determining length of times
if glofasfiles is None:
   try:
     import cdsapi
   except ModuleNotFoundError as e:
     raise ModuleNotFoundError('cdsapi not available, please download glofas separately and specify datafile at command line') from e
   bnds = [np.malonN(latN)+0.1,np.min(lonN)-0.1,np.min(latN)-0.1,np.malonN(lonN)+0.1]
   # download GLOFAS forecast data
   dataset = "cems-glofas-forecast"
   request = {
     "system_version": ["operational"],
     "hydrological_model": ["lisflood"],
     "product_type": ["control_forecast"],
     "variable": "river_discharge_in_the_last_24_hours",
     "year": [mdate[0]],
     "month": [mdate[1]],
     "day": [mdate[2]],
     "leadtime_hour": [ '%s' % (24*d) for d in range(1,rnday+1)
     ],
     "data_format": "netcdf",
     "download_format": "unarchived",
     "area": bnds
   }
   fname = os.path.join(outdir,'GLOFAS-today-full.nc')
   client = cdsapi.Client(url='https://ewds.climate.copernicus.eu/api')
   req = client.retrieve(dataset, request, fname)
   glofasfiles = [fname] 

for fname in glofasfiles:
   if fcast:
      ncid = nc.Dataset(fname)
      p0 = np.searchsorted(lat, ncid.variables['latitude'][0], sorter=np.arange(len(lat)-1, -1, -1)) + 1 # lat is in decreasing order
      p1 = np.searchsorted(lon, ncid.variables['longitude'][0]) -1
      print(p0,p1,lon,lat,ncid.variables['latitude'][0], ncid.variables['longitude'][0] )
      ind0N_sub = ind0N - (len(lat)-p0)
      ind1N_sub = ind1N - p1
      g = np.where(np.logical_and.reduce((ind0N_sub>=0, ind1N_sub>=0, ind0N_sub < ncid.variables['latitude'].shape[0], ind1N_sub < ncid.variables['longitude'].shape[0])))[0]
      print(f'found {g.shape} locations in area, out of {ind0N.shape}')
      Rlon = ncid.variables['longitude'][ind1N_sub[g]]
      Rlat = ncid.variables['latitude'][ind0N_sub[g]]
      riverCoordsSubset[g,:] = np.c_[Rlon,Rlat]
      T = ncid.variables['valid_time'][:]
      time_units = ncid.variables['valid_time'].units
      try:
         GLOFASstart = datetime.datetime.strptime(time_units,'seconds since %Y-%m-%dT%H:%M:%S')
      except:
         GLOFASstart = datetime.datetime.strptime(time_units,'seconds since %Y-%m-%d')
      GLOFASreftime = ncid.variables['forecast_reference_time'][0]
      relmodstart = (modelstartdate - GLOFASstart).total_seconds()
      relmodend = (modelenddate - GLOFASstart).total_seconds()
      if len(T.shape)>0:
         timeind = np.argwhere(np.logical_and(T>=relmodstart,T<=relmodend)).ravel()
         T = T[timeind]-relmodstart
      else:
         timeind = np.array([0])
         T = np.array([T.data-relmodstart])
      discharge = np.zeros((timeind.shape[0],ind0N.shape[0])) * np.nan
      dis24 = ncid.variables['dis24'][timeind,0,:,:]
      for i in g:
        discharge[:,i] = dis24[:,ind0N_sub[i],ind1N_sub[i]]
   else:
      ncid = nc.Dataset(fname)
      print(fname)
      py = np.searchsorted(lat, ncid.variables['latitude'][0], sorter=np.arange(len(lat)-1, -1, -1)) + 1 # lat is in decreasing order
      px = np.searchsorted(lon, ncid.variables['longitude'][0]) -1
      xindN_sub = ind1N - px
      yindN_sub = ind0N - (len(lat)-py)
      xindN = ind1N
      yindN = ind0N
      g = np.where(np.logical_and.reduce((xindN_sub>=0, yindN_sub>=0, xindN_sub < ncid.variables['longitude'].shape[0], yindN_sub < ncid.variables['latitude'].shape[0])))[0]
      Rlon = ncid.variables['longitude'][xindN_sub[g]]
      Rlat = ncid.variables['latitude'][yindN_sub[g]]
      riverCoordsSubset[g,:] = np.c_[Rlon,Rlat]
      if discharge is None:
        T = ncid.variables['valid_time'][:]
        time_units = ncid.variables['valid_time'].units
        GLOFASstart = datetime.datetime.strptime(time_units,'seconds since %Y-%m-%d')
        relmodstart = (modelstartdate - GLOFASstart).total_seconds()
        relmodend = (modelenddate - GLOFASstart).total_seconds()
        timeind = np.argwhere(np.logical_and(T>=relmodstart,T<=relmodend)).ravel()
        discharge = np.zeros((timeind.shape[0],xindN.shape[0])) * np.nan
        T = T[timeind]-relmodstart
      dis24 = ncid.variables['dis24'][timeind,:,:]
      for i in g:
        discharge[:,i] = dis24[:,yindN_sub[i],xindN_sub[i]]


while T[0] > 0:
   T = np.r_[T[0]-86400,T]
   discharge = np.r_[discharge[0,:][np.newaxis,:], discharge]
   print(f'WARNING: mismatch between start of GloFAS data and run starttime, padding start of files with first available GloFAS data.')
while T[-1] < relmodend-relmodstart:
   T = np.r_[T,T[-1]+86400]
   discharge = np.r_[discharge,discharge[-1,:][np.newaxis,:]]
   print(f'WARNING: mismatch between end of GloFAS data and run endtime, padding end of files with last available GloFAS data.')
maxdischarge = np.max(discharge,axis=0)
# test plot showing successful alignment of main grid and subgrid methods
for plotarea in plotbounds.keys():
  f,ax = plt.subplots(1,1)
  ax.plot(riverCoordsSubset[:,0],riverCoordsSubset[:,1],'+m')
  c = ax.scatter(lonN,latN,np.log2(areaN)*8,np.log10(areaN))
  plt.colorbar(c,label='log2 watershed area')
  ax.axis(plotbounds[plotarea])
  f.savefig(os.path.join(imagedir,f'glofas_{plotarea}_{tag}_watershedarea.png'))
  f,ax = plt.subplots(1,1)
  ax.plot(riverCoordsSubset[:,0],riverCoordsSubset[:,1],'+m')
  c = ax.scatter(lonN,latN,np.log2(maxdischarge)*8,np.log10(maxdischarge))
  plt.colorbar(c,label='log2 discharge')
  ax.plot(xctr[Erivers],yctr[Erivers],'kd')
  ax.axis(plotbounds[plotarea])
  f.savefig(os.path.join(imagedir,f'glofas_{plotarea}_{tag}_end.png'))
  dx = [1,10, 100,500,1e6]
  cs = ['y','c','g','b','r']
  plt.close(f)
  f,ax = plt.subplots(1,1)
  for i in range(len(dx)-1):
     gb =  np.logical_and.reduce((areaN>=dx[i], areaN<dx[i+1]))
     ax.plot(T/86400,np.sum(discharge[:,gb],axis=1),c=cs[i],label=f'{dx[i]} - {dx[i+1]}')
  ax.legend(title=f'discharge range in {plotarea}')
  ax.set_xlabel('days')
  ax.set_ylabel('Q (m^3/s)')
  f.savefig(os.path.join(imagedir,f'glofas_{plotarea}_{tag}_discharge_by_area.png'))
  plt.close(f)
  f,ax = plt.subplots(1,1)
  for i in range(len(dx)-1):
     gb =  np.logical_and.reduce((areaN>=dx[i], areaN<dx[i+1]))
     ax.plot(np.sum(areaN[gb]),np.max(np.sum(discharge[:,gb],axis=1)),c=cs[i],label=f'{dx[i]} - {dx[i+1]}',marker='.')
  ax.legend(title=f'discharge range in {plotarea}')
  ax.set_xlabel('area (km)')
  ax.set_ylabel('max Q (m^3/s)')
  f.savefig(os.path.join(imagedir,f'glofas_{plotarea}_{tag}_mxdischarge_vs_area.png'))

# merge river values from duplicate elements
dischargeE = np.zeros((discharge.shape[0],uniqErivers.shape[0]))
dischargeE[:,singles] = discharge[:,uniqEriversInd[singles]]
for d in dups:
   ind = np.where(Erivers == uniqErivers[d])[0]
   for i in ind:
       dischargeE[:,d] += discharge[:,i]

empty = np.unique(np.argwhere(np.isnan(dischargeE))[:,1])
if empty.shape[0]>0:
   # write vsource.bp as sanity check
  props = np.zeros((empty.shape[0],4))
  props[:,0] = np.arange(empty.shape[0])+1
  props[:,1:3] = np.c_[xctr[uniqErivers[empty]],yctr[uniqErivers[empty]]]
  with open(os.path.join(outdir,f'{tag}empty_rivers.bp'),'w') as f:
    f.write('GLOFAS river locations not covered in data files\n%s\n' % props.shape[0])
    np.savetxt(f,props,fmt='%d %.8f %.8f %.1f')
  dischargeE[:,empty] = 0  # fill missing rivers with 0 flow to avoid null values in .th files

# merge NWM and GLOFAS

def multiInterp2(x, xp, fp):
    '''simulataneous 1d interp for multiple y rows
    derived from: https://stackoverflow.com/questions/43772218/fastest-way-to-use-numpy-interp-on-a-2-d-array'''
    i = np.arange(fp.shape[1])
    ii = np.broadcast_to(i[np.newaxis,:],(len(x),len(i)))
    j = np.searchsorted(xp,x)
    j[j==len(xp)] -=1
    j[j > 0] -=1
    jj = np.broadcast_to(j[:,np.newaxis],(len(x),len(i)))
    xx = np.broadcast_to(x[:,np.newaxis],(len(x),len(i)))
    d = (xx - xp[jj]) / (xp[jj + 1] - xp[jj])
    return (1 - d) * fp[jj,ii] + fp[jj+1,ii] * d

dischargeE = multiInterp2(nwmtimes,T,dischargeE)
allSourceE = np.r_[uniqErivers, np.array(eid_sources, dtype=int)-1] # convert NWM Eids from 1 based names to 0 based int
allSources = np.c_[dischargeE,nwmsources]
eid_sinks = np.array(eid_sinks, dtype = int)-1 # convert from 1 based names to 0 based int

# write output files containing 1-indexed list of elements source_sink.in, source_sink.prop file, and source_sink.bp file (2nd and 3rd for testing)
with open(os.path.join(outdir,f'{tag}source_sink.in'),'w') as f:
  f.write('%d\n' % len(allSourceE))
  np.savetxt(f,allSourceE+1,fmt='%d')
  f.write('\n')
  f.write('%d\n' % len(eid_sinks))
  np.savetxt(f,eid_sinks+1,fmt='%d')

# write vsource.th from discharge and valid_time (converted to seconds since start of run)
vsource = np.c_[nwmtimes,allSources]
with open(os.path.join(outdir,f'{tag}vsource.th'),'w') as f:
  np.savetxt(f,vsource)

vsink = np.c_[nwmtimes,nwmsinks]
with open(os.path.join(outdir,f'{tag}vsink.th'),'w') as f:
  np.savetxt(f,vsink)

# write msource.th from discharge and valid_time (converted to seconds since start of run)
salval = 0 
msource = np.c_[nwmtimes,allSources*0+salval]
with open(os.path.join(outdir,f'{tag}msource.th'),'w') as f:
  np.savetxt(f,msource)

# write vsource.prop as sanity check
props = np.zeros((ne,2),dtype = int)
props[:,0] = np.arange(ne)+1
props[allSourceE,1] = (np.round(np.max(allSources,axis=0))).astype(int)
with open(os.path.join(outdir,f'{tag}vsource.prop'),'w') as f:
  np.savetxt(f,props,fmt='%d')

# write vsource.bp as sanity check
props = np.zeros((len(allSourceE),4))
props[:,0] = np.arange(len(allSourceE))+1
props[:,1:3] = np.c_[xctr[allSourceE],yctr[allSourceE]]
props[:,3] = np.max(allSources,axis=0)
with open(os.path.join(outdir,f'{tag}vsource.bp'),'w') as f:
  f.write('GLOFAS+NWM river locations\n%s\n' % props.shape[0])
  np.savetxt(f,props,fmt='%d %.8f %.8f %.1f')

# write vsink.bp as sanity check
props = np.zeros((len(eid_sinks),4))
props[:,0] = np.arange(len(eid_sinks))+1
props[:,1:3] = np.c_[xctr[eid_sinks],yctr[eid_sinks]]
props[:,3] = np.max(eid_sinks,axis=0)
with open(os.path.join(outdir,f'{tag}vsink.bp'),'w') as f:
  f.write('NWM river sink locations\n%s\n' % props.shape[0])
  np.savetxt(f,props,fmt='%d %.8f %.8f %.1f')

xuniq = xctr[allSourceE]
yuniq = yctr[allSourceE]
if processplot:
  vanc = np.where(np.logical_and.reduce((xuniq < plotbounds[plotarea][0],xuniq>plotbounds[plotarea][1], yuniq >plotbounds[plotarea][2], yuniq < plotbounds[plotarea][3])))[0]
  f,ax = plt.subplots(1,1)
  ax.plot(nwmtimes,allSources[:,vanc],c='grey')
  ax.set_title(f'GLOFAS+NWM discharge around {plotarea}')
  f.savefig(os.path.join(imagedir,f'sources_{plotarea}_{tag}_tseries.png'))
  plt.close(f)

