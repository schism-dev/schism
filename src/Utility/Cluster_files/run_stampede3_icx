#!/bin/bash
#----------------------------------------------------
# Sample Slurm job script
#   for TACC Stampede3 ICX nodes
#
#   *** MPI Job in ICX Queue ***
# 
# Last revised: 23 April 2024
#
# Notes:
#
#   -- Launch this script by executing
#      "sbatch icx.mpi.slurm" on Stampede3 login node.
#
#   -- Use ibrun to launch MPI codes on TACC systems.
#      Do not use mpirun or mpiexec.
#
#   -- Max recommended MPI ranks per ICX node: 80
#      (start small, increase gradually).
#
#   -- If you're running out of memory, try running
#      on more nodes using fewer tasks and/or threads 
#      per node to give each task access to more memory.
#
#   -- Don't worry about task layout.  By default, ibrun
#      will provide proper affinity and pinning.
#
#   -- You should always run out of $SCRATCH.  Your input
#      files, output files, and exectuable should be 
#      in the $SCRATCH directory hierarchy.
#
#----------------------------------------------------

#SBATCH -J 04s             # Job name
#SBATCH -o myjob.o%j       # Name of stdout output file
#SBATCH -e myjob.e%j       # Name of stderr error file
#SBATCH -p icx             # Queue (partition) name
##Max 32 nodes (80cores/node), 48hrs
#SBATCH -N 6               # Total # of nodes 
#SBATCH -n 480             # Total # of mpi tasks 
#SBATCH -t 24:00:00        # Run time (hh:mm:ss)
#SBATCH --mail-user=yjzhang@vims.edu
#SBATCH --mail-type=all    # Send email at begin and end of job
#SBATCH -A OCE24002

# Other commands must follow all #SBATCH directives...
module load netcdf/4.9.2
module list
pwd
date

# Always run your jobs out of $SCRATCH.  Your input files, output files,
# and exectuable should be in the $SCRATCH directory hierarchy.
# Change directories to your $SCRATCH directory where your executable is

#cd $SCRATCH/External_Exchanges/EX84/RUN04s

# Launch MPI code...

ibrun ./pschism_STAM3_PREC_EVAP_BLD_STANDALONE_SH_MEM_COMM_TVD-VL 6
